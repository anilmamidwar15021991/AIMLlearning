{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1f1a57-d074-4bb3-b618-eafdc0ee5d19",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c90e375-f29d-4015-aff6-e5762925d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35145064-2be4-47bf-8a7e-92fce82dfcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"spam.csv\",encoding='latin1')\n",
    "df=df.rename(columns={'v1':'label','v2':'text'})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32af62a0-25b9-4651-9572-fd31917eb7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8dfb17-a42d-4f6d-8adf-515d1b14ad6d",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "* tokenization\n",
    "* stopwords\n",
    "* stemming/lemmetaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068aea48-1fd4-4198-ba39-4333cfdf8818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e0d9df0-3eae-43f4-860d-2b5748df2184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1    0\n",
       "v2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check for missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e01cc8a7-43c1-4e05-a85e-61197e91ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"\\b\\w*t\\w*'t\\b\" #checking words like won't, haven't , isn't etc\n",
    "matches=df['text'].apply(lambda x: re.findall(pattern,x) if len(re.findall(pattern,x))>0 else None)\n",
    "matches.notna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebf0d5c4-fb78-419f-9f55-956abe19df75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will Ì_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5eaa56d-25b7-4cb0-b7bf-7ec0619f73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "import string\n",
    "def text_processing(message):\n",
    "    try:\n",
    "        corpus=[]\n",
    "        message=message.lower()\n",
    "        message=re.sub('[^a-zA-Z0-9]+',' ',message) #specail character regex\n",
    "        #message=re.sub('\\S*\\d\\S*','',message).strip() #only alphabet regex\n",
    "        \n",
    "        #message=message.split() and nltk.word_tokenize does same same work \n",
    "        #tokenization -sentence to words\n",
    "        message=nltk.word_tokenize(message)\n",
    "    \n",
    "        for i in message:\n",
    "            if i.isalnum():\n",
    "                corpus.append(i)\n",
    "        message=corpus[:]\n",
    "        corpus.clear()\n",
    "    \n",
    "    #removing stopwords and punctuations\n",
    "        for i in message:\n",
    "            if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "                corpus.append(i.strip())\n",
    "    \n",
    "        message=corpus[:]\n",
    "        corpus.clear()\n",
    "        #stemming applied on text\n",
    "        for i in message:\n",
    "            corpus.append(ps.stem(i))\n",
    "    \n",
    "        return corpus\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(message)\n",
    "        raise e\n",
    "df['processed_text']=df['text'].apply(text_processing)\n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "080737a6-841d-4462-b4db-3391dfb1e6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
       "1                       [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4       [nah, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1be4875c-fb4a-40ac-9e49-0da5acac3697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 30638)\n"
     ]
    }
   ],
   "source": [
    "#creating the bag of words (words to vectors)\n",
    "from wordcloud import WordCloud \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv=CountVectorizer(ngram_range=(2,2),binary=True)\n",
    "X=cv.fit_transform(df['processed_text'].astype('str')).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc724597-de21-4166-82c0-6de75d4cd483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting categorical value of label column to number\n",
    "y=pd.get_dummies(df['label'])\n",
    "y=y.iloc[:,1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4904a7ab-e901-4758-b1cc-4faf445f205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\anaconda3\\envs\\Venv_Learning\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 per</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>000 bonu</th>\n",
       "      <th>000 cash</th>\n",
       "      <th>000 homeown</th>\n",
       "      <th>000 pound</th>\n",
       "      <th>000 price</th>\n",
       "      <th>000 prize</th>\n",
       "      <th>000 xma</th>\n",
       "      <th>...</th>\n",
       "      <th>zed pobox</th>\n",
       "      <th>zero save</th>\n",
       "      <th>zhong se</th>\n",
       "      <th>zindgi wo</th>\n",
       "      <th>zoe 18</th>\n",
       "      <th>zoe hit</th>\n",
       "      <th>zogtoriu stare</th>\n",
       "      <th>zoom cine</th>\n",
       "      <th>zouk nichol</th>\n",
       "      <th>zyada kisi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30638 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00 easter  00 per  00 sub  000 bonu  000 cash  000 homeown  000 pound  \\\n",
       "0          0       0       0         0         0            0          0   \n",
       "1          0       0       0         0         0            0          0   \n",
       "2          0       0       0         0         0            0          0   \n",
       "3          0       0       0         0         0            0          0   \n",
       "4          0       0       0         0         0            0          0   \n",
       "\n",
       "   000 price  000 prize  000 xma  ...  zed pobox  zero save  zhong se  \\\n",
       "0          0          0        0  ...          0          0         0   \n",
       "1          0          0        0  ...          0          0         0   \n",
       "2          0          0        0  ...          0          0         0   \n",
       "3          0          0        0  ...          0          0         0   \n",
       "4          0          0        0  ...          0          0         0   \n",
       "\n",
       "   zindgi wo  zoe 18  zoe hit  zogtoriu stare  zoom cine  zouk nichol  \\\n",
       "0          0       0        0               0          0            0   \n",
       "1          0       0        0               0          0            0   \n",
       "2          0       0        0               0          0            0   \n",
       "3          0       0        0               0          0            0   \n",
       "4          0       0        0               0          0            0   \n",
       "\n",
       "   zyada kisi  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 30638 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X and y are converted to numerical of array , lets check X features \n",
    "X_features=pd.DataFrame(X,columns=cv.get_feature_names())\n",
    "X_features.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e007d15b-51c7-4e3d-b259-d09937dae7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e912b5fb-b72c-4a88-8b6b-4737c792d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model=MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c47bf253-1382-4a5c-a56a-c116f8232cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b998e0f-8c35-4d08-baa8-421b059a134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8663677130044843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073684ab-1723-4b4c-a33e-bae56094f496",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf649fe1-49bb-4f6a-aaae-60e3fad3bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,keyedvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "509a255d-0536-4c90-bf33-bb711c79bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def text_processing(message):\n",
    "    try:\n",
    "        corpus=[]\n",
    "        message=message.lower()\n",
    "        message=re.sub('[^a-zA-Z0-9]+',' ',message) #specail character regex\n",
    "        #message=re.sub('\\S*\\d\\S*','',message).strip() #only alphabet regex\n",
    "        \n",
    "        #message=message.split() and nltk.word_tokenize does same same work \n",
    "        #tokenization -sentence to words\n",
    "        message=nltk.word_tokenize(message)\n",
    "    \n",
    "        for i in message:\n",
    "            if i.isalnum():\n",
    "                corpus.append(i)\n",
    "        message=corpus[:]\n",
    "        corpus.clear()\n",
    "    \n",
    "    #removing stopwords and punctuations\n",
    "        for i in message:\n",
    "            if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "                corpus.append(i.strip())\n",
    "    \n",
    "        message=corpus[:]\n",
    "        corpus.clear()\n",
    "        #lemmatization  applied on text\n",
    "        for i in message:\n",
    "            corpus.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "        return corpus\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(message)\n",
    "        raise e\n",
    "df['processed_text_Lemmatize']=df['text'].apply(text_processing)\n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "656886cc-4159-46a5-b699-ed992fb7f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "959\n",
      "2805\n",
      "3374\n",
      "4115\n",
      "4573\n",
      "4822\n",
      "5572\n"
     ]
    }
   ],
   "source": [
    "#wordtovec - create one single list for all messages\n",
    "i=0\n",
    "list_of_messsages=[]\n",
    "for i , mesg in enumerate(df['processed_text_Lemmatize']):\n",
    "    if len(mesg)==0:  #printing index where list in empty\n",
    "        print(i)\n",
    "    list_of_messsages.append(mesg)\n",
    "print(len(list_of_messsages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9377cbb-00ba-4d5b-9069-e6f5831b2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create word2vec model from all the words present in list_of_messsages\n",
    "ram_gt_16g=True\n",
    "use_google_w2v = False\n",
    "train_w2v=True\n",
    "if train_w2v:\n",
    "    w2vec_model=Word2Vec(list_of_messsages,min_count=5,vector_size=50,window=5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75d046a3-b32b-4fd7-99ca-9eaa02c86bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'call',\n",
       " '2',\n",
       " 'get',\n",
       " 'ur',\n",
       " '4',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'go',\n",
       " 'ok',\n",
       " 'free',\n",
       " 'day',\n",
       " 'know',\n",
       " 'got',\n",
       " 'come',\n",
       " 'like',\n",
       " 'good',\n",
       " 'time',\n",
       " 'text',\n",
       " 'love',\n",
       " 'want',\n",
       " 'send',\n",
       " 'need',\n",
       " 'one',\n",
       " 'today',\n",
       " 'txt',\n",
       " 'r',\n",
       " '1',\n",
       " 'going',\n",
       " 'home',\n",
       " 'c',\n",
       " 'stop',\n",
       " 'lor',\n",
       " 'sorry',\n",
       " 'see',\n",
       " 'still',\n",
       " 'take',\n",
       " 'mobile',\n",
       " 'n',\n",
       " 'back',\n",
       " 'da',\n",
       " 'reply',\n",
       " 'k',\n",
       " 'think',\n",
       " 'dont',\n",
       " 'tell',\n",
       " 'week',\n",
       " 'hi',\n",
       " 'phone',\n",
       " 'new',\n",
       " 'later',\n",
       " 'pls',\n",
       " 'please',\n",
       " 'co',\n",
       " 'msg',\n",
       " 'make',\n",
       " 'dear',\n",
       " 'night',\n",
       " 'message',\n",
       " 'say',\n",
       " 'well',\n",
       " 'thing',\n",
       " 'much',\n",
       " 'min',\n",
       " 'claim',\n",
       " 'great',\n",
       " 'hope',\n",
       " 'oh',\n",
       " 'hey',\n",
       " 'number',\n",
       " 'na',\n",
       " '3',\n",
       " 'happy',\n",
       " 'friend',\n",
       " 'wat',\n",
       " 'work',\n",
       " 'give',\n",
       " 'yes',\n",
       " 'way',\n",
       " 'www',\n",
       " 'let',\n",
       " 'e',\n",
       " 'prize',\n",
       " 'right',\n",
       " 'tomorrow',\n",
       " 'wan',\n",
       " 'already',\n",
       " 'ask',\n",
       " 'said',\n",
       " 'cash',\n",
       " 'yeah',\n",
       " 'really',\n",
       " 'tone',\n",
       " 'life',\n",
       " 'b',\n",
       " 'amp',\n",
       " 'meet',\n",
       " 'babe',\n",
       " 'im',\n",
       " 'find',\n",
       " 'win',\n",
       " 'morning',\n",
       " 'miss',\n",
       " 'last',\n",
       " 'uk',\n",
       " 'service',\n",
       " 'thanks',\n",
       " 'would',\n",
       " 'year',\n",
       " 'also',\n",
       " 'anything',\n",
       " 'lol',\n",
       " 'feel',\n",
       " 'care',\n",
       " 'keep',\n",
       " 'every',\n",
       " 'sure',\n",
       " 'pick',\n",
       " '150p',\n",
       " 'com',\n",
       " 'urgent',\n",
       " 'contact',\n",
       " 'nokia',\n",
       " 'sent',\n",
       " 'something',\n",
       " 'buy',\n",
       " 'wait',\n",
       " 'cant',\n",
       " '5',\n",
       " 'place',\n",
       " 'first',\n",
       " 'nice',\n",
       " 'someone',\n",
       " 'went',\n",
       " 'guy',\n",
       " '50',\n",
       " 'even',\n",
       " 'wish',\n",
       " 'next',\n",
       " 'tonight',\n",
       " 'around',\n",
       " 'word',\n",
       " 'show',\n",
       " 'soon',\n",
       " 'could',\n",
       " 'customer',\n",
       " 'money',\n",
       " 'many',\n",
       " 'help',\n",
       " 'gon',\n",
       " 'sleep',\n",
       " 'late',\n",
       " 'chat',\n",
       " 'ya',\n",
       " 'per',\n",
       " 'always',\n",
       " 'sm',\n",
       " 'leave',\n",
       " 'v',\n",
       " 'lot',\n",
       " 'gud',\n",
       " 'dun',\n",
       " 'x',\n",
       " 'told',\n",
       " 'name',\n",
       " 'waiting',\n",
       " 'special',\n",
       " 'hello',\n",
       " '16',\n",
       " 'end',\n",
       " 'fine',\n",
       " 'girl',\n",
       " 'haha',\n",
       " '18',\n",
       " 'people',\n",
       " 'hour',\n",
       " 'minute',\n",
       " 'person',\n",
       " 'coming',\n",
       " 'heart',\n",
       " 'yet',\n",
       " 'thk',\n",
       " 'may',\n",
       " 'getting',\n",
       " 'guaranteed',\n",
       " 'smile',\n",
       " 'done',\n",
       " 'try',\n",
       " 'use',\n",
       " 'month',\n",
       " 'thought',\n",
       " '6',\n",
       " 'offer',\n",
       " 'god',\n",
       " '7',\n",
       " 'talk',\n",
       " 'holiday',\n",
       " 'class',\n",
       " 'best',\n",
       " 'cost',\n",
       " 'man',\n",
       " 'mean',\n",
       " 'car',\n",
       " 'stuff',\n",
       " 'lunch',\n",
       " 'live',\n",
       " 'line',\n",
       " 'start',\n",
       " 'finish',\n",
       " '1000',\n",
       " 'job',\n",
       " 'draw',\n",
       " 'problem',\n",
       " '500',\n",
       " 'bit',\n",
       " 'trying',\n",
       " 'better',\n",
       " 'thats',\n",
       " 'meeting',\n",
       " 'never',\n",
       " 'house',\n",
       " 'plan',\n",
       " 'yup',\n",
       " 'dat',\n",
       " 'account',\n",
       " 'ready',\n",
       " 'long',\n",
       " 'ill',\n",
       " 'cool',\n",
       " 'rate',\n",
       " '100',\n",
       " 'weekend',\n",
       " 'chance',\n",
       " '10',\n",
       " 'mind',\n",
       " 'game',\n",
       " 'latest',\n",
       " 'enjoy',\n",
       " 'world',\n",
       " 'play',\n",
       " 'sir',\n",
       " 'half',\n",
       " 'real',\n",
       " 'yo',\n",
       " 'wk',\n",
       " 'room',\n",
       " 'check',\n",
       " 'guess',\n",
       " 'camera',\n",
       " 'eat',\n",
       " '1st',\n",
       " 'voucher',\n",
       " 'receive',\n",
       " 'look',\n",
       " 'pic',\n",
       " 'nothing',\n",
       " 'awarded',\n",
       " 'boy',\n",
       " 'lar',\n",
       " 'liao',\n",
       " 'another',\n",
       " 'shit',\n",
       " 'sweet',\n",
       " 'big',\n",
       " 'landline',\n",
       " 'ah',\n",
       " 'birthday',\n",
       " 'dinner',\n",
       " 'ever',\n",
       " 'kiss',\n",
       " 'box',\n",
       " 'quite',\n",
       " 'might',\n",
       " 'video',\n",
       " 'luv',\n",
       " 'jus',\n",
       " 'bt',\n",
       " '150ppm',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'question',\n",
       " 'called',\n",
       " 'tv',\n",
       " 'aight',\n",
       " 'worry',\n",
       " 'orange',\n",
       " 'early',\n",
       " 'point',\n",
       " 'pa',\n",
       " 'xxx',\n",
       " 'speak',\n",
       " 'po',\n",
       " 'probably',\n",
       " 'baby',\n",
       " 'den',\n",
       " 'apply',\n",
       " 'sat',\n",
       " 'bed',\n",
       " 'fun',\n",
       " 'shall',\n",
       " 'hear',\n",
       " 'remember',\n",
       " 'two',\n",
       " 'network',\n",
       " 'wont',\n",
       " 'part',\n",
       " 'ringtone',\n",
       " 'princess',\n",
       " 'maybe',\n",
       " 'thanx',\n",
       " 'bad',\n",
       " 'forgot',\n",
       " 'actually',\n",
       " 'code',\n",
       " 'left',\n",
       " 'shopping',\n",
       " 'office',\n",
       " '9',\n",
       " 'dream',\n",
       " 'reach',\n",
       " 'easy',\n",
       " 'bus',\n",
       " 'dunno',\n",
       " 'pay',\n",
       " 'fuck',\n",
       " 'leh',\n",
       " 'dis',\n",
       " 'put',\n",
       " 'wife',\n",
       " 'face',\n",
       " 'hurt',\n",
       " 'dad',\n",
       " 'little',\n",
       " '2nd',\n",
       " 'evening',\n",
       " 'didnt',\n",
       " 'afternoon',\n",
       " 'school',\n",
       " 'looking',\n",
       " 'made',\n",
       " 'thank',\n",
       " 'mate',\n",
       " '000',\n",
       " 'selected',\n",
       " 'sound',\n",
       " '8',\n",
       " 'award',\n",
       " 'everything',\n",
       " 'enough',\n",
       " 'working',\n",
       " 'mail',\n",
       " 'movie',\n",
       " 'anyway',\n",
       " 'detail',\n",
       " 'town',\n",
       " 'without',\n",
       " 'collect',\n",
       " 'asked',\n",
       " 'tmr',\n",
       " 'missing',\n",
       " 'true',\n",
       " 'pound',\n",
       " 'since',\n",
       " 'wanted',\n",
       " 'must',\n",
       " 'wif',\n",
       " 'though',\n",
       " 'okay',\n",
       " 'join',\n",
       " 'sexy',\n",
       " 'g',\n",
       " 'xmas',\n",
       " 'came',\n",
       " 'run',\n",
       " 'answer',\n",
       " 'lesson',\n",
       " 'entry',\n",
       " 'update',\n",
       " 'abt',\n",
       " 'price',\n",
       " 'important',\n",
       " 'gift',\n",
       " 'wot',\n",
       " 'pain',\n",
       " 'til',\n",
       " 'hav',\n",
       " 'able',\n",
       " 'bring',\n",
       " 'wake',\n",
       " 'mob',\n",
       " 'collection',\n",
       " 'book',\n",
       " 'missed',\n",
       " 'away',\n",
       " 'date',\n",
       " 'de',\n",
       " 'test',\n",
       " 'juz',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'colour',\n",
       " 'charge',\n",
       " 'decimal',\n",
       " 'change',\n",
       " '2000',\n",
       " 'stay',\n",
       " '5000',\n",
       " 'dude',\n",
       " '10p',\n",
       " 'music',\n",
       " 'double',\n",
       " 'attempt',\n",
       " 'yesterday',\n",
       " 'weekly',\n",
       " 'alright',\n",
       " 'wen',\n",
       " 'havent',\n",
       " 'till',\n",
       " 'saw',\n",
       " 'valid',\n",
       " 'else',\n",
       " 'hair',\n",
       " 'drink',\n",
       " 'id',\n",
       " 'making',\n",
       " 'net',\n",
       " 'online',\n",
       " 'bored',\n",
       " 'top',\n",
       " 'oso',\n",
       " 'trip',\n",
       " 'friendship',\n",
       " 'credit',\n",
       " 'shop',\n",
       " 'food',\n",
       " 'haf',\n",
       " 'lei',\n",
       " 'player',\n",
       " 'driving',\n",
       " '800',\n",
       " 'nite',\n",
       " 'feeling',\n",
       " 'national',\n",
       " 'sch',\n",
       " 'address',\n",
       " 'delivery',\n",
       " 'coz',\n",
       " 'either',\n",
       " 'tried',\n",
       " 'ard',\n",
       " 'family',\n",
       " 'goin',\n",
       " 'yr',\n",
       " 'hot',\n",
       " 'nt',\n",
       " 'together',\n",
       " 'full',\n",
       " 'wid',\n",
       " 'sae',\n",
       " 'mom',\n",
       " 'ring',\n",
       " 'lose',\n",
       " 'order',\n",
       " 'second',\n",
       " 'sad',\n",
       " 'brother',\n",
       " '8007',\n",
       " 'post',\n",
       " 'busy',\n",
       " 'head',\n",
       " 'bonus',\n",
       " 'si',\n",
       " 'calling',\n",
       " 'story',\n",
       " 'believe',\n",
       " 'tot',\n",
       " 'http',\n",
       " 'smoke',\n",
       " 'beautiful',\n",
       " 'happen',\n",
       " 'eve',\n",
       " 'mum',\n",
       " 'smiling',\n",
       " 'club',\n",
       " 'drive',\n",
       " 'noe',\n",
       " 'await',\n",
       " 'sleeping',\n",
       " 'saying',\n",
       " 'close',\n",
       " 'w',\n",
       " 'huh',\n",
       " 'chikku',\n",
       " 'walk',\n",
       " 'old',\n",
       " 'row',\n",
       " 'set',\n",
       " 'leaving',\n",
       " 'cause',\n",
       " 'poly',\n",
       " 'started',\n",
       " 'gr8',\n",
       " 'news',\n",
       " 'ta',\n",
       " 'simple',\n",
       " 'awesome',\n",
       " 'took',\n",
       " 'drop',\n",
       " 'okie',\n",
       " 'parent',\n",
       " 'aft',\n",
       " '86688',\n",
       " 'hand',\n",
       " 'private',\n",
       " '750',\n",
       " 'tomo',\n",
       " 'congrats',\n",
       " 'finished',\n",
       " 'thinking',\n",
       " 'land',\n",
       " 'rite',\n",
       " 'email',\n",
       " 'pub',\n",
       " 'match',\n",
       " 'pm',\n",
       " 'card',\n",
       " '150',\n",
       " 'reason',\n",
       " 'content',\n",
       " 'statement',\n",
       " 'caller',\n",
       " 'gd',\n",
       " 'wil',\n",
       " 'valentine',\n",
       " 'available',\n",
       " 'neva',\n",
       " 'tho',\n",
       " 'company',\n",
       " 'break',\n",
       " 'touch',\n",
       " 'age',\n",
       " 'forget',\n",
       " '250',\n",
       " 'sister',\n",
       " 'mine',\n",
       " 'final',\n",
       " 'taking',\n",
       " 'tc',\n",
       " 'auction',\n",
       " 'everyone',\n",
       " 'unsubscribe',\n",
       " 'angry',\n",
       " 'whats',\n",
       " 'open',\n",
       " 'opt',\n",
       " 'ticket',\n",
       " 'dating',\n",
       " 'whatever',\n",
       " '11',\n",
       " 'expires',\n",
       " 'found',\n",
       " 'knw',\n",
       " 'choose',\n",
       " 'fancy',\n",
       " '12hrs',\n",
       " 'search',\n",
       " 'lucky',\n",
       " 'alone',\n",
       " 'xx',\n",
       " 'worth',\n",
       " 'anyone',\n",
       " 'bank',\n",
       " 'carlos',\n",
       " 'treat',\n",
       " 'type',\n",
       " 'gal',\n",
       " 'loving',\n",
       " 'prob',\n",
       " 'welcome',\n",
       " 'pobox',\n",
       " 'friday',\n",
       " 'uncle',\n",
       " 'ha',\n",
       " 'frnd',\n",
       " 'kind',\n",
       " 'exam',\n",
       " 'congratulation',\n",
       " 'song',\n",
       " 'winner',\n",
       " 'quiz',\n",
       " 'anytime',\n",
       " 'hit',\n",
       " 'college',\n",
       " 'visit',\n",
       " 'mobileupd8',\n",
       " 'sun',\n",
       " 'hard',\n",
       " 'decided',\n",
       " 'identifier',\n",
       " 'boytoy',\n",
       " 'smth',\n",
       " 'far',\n",
       " '30',\n",
       " 'happened',\n",
       " 'secret',\n",
       " 'gone',\n",
       " 'saturday',\n",
       " 'fast',\n",
       " 'info',\n",
       " 'ltd',\n",
       " 'party',\n",
       " '08000930705',\n",
       " 'fucking',\n",
       " 'wonderful',\n",
       " 'light',\n",
       " 'th',\n",
       " 'mu',\n",
       " 'wit',\n",
       " 'project',\n",
       " 'chennai',\n",
       " 'wrong',\n",
       " 'crazy',\n",
       " 'hows',\n",
       " 'finally',\n",
       " '2lands',\n",
       " 'bout',\n",
       " 'suite342',\n",
       " 'darlin',\n",
       " 'goodmorning',\n",
       " 'oredi',\n",
       " 'fri',\n",
       " 'lovely',\n",
       " 'wonder',\n",
       " '350',\n",
       " '08000839402',\n",
       " 'camcorder',\n",
       " 'tel',\n",
       " 'drug',\n",
       " 'nope',\n",
       " 'operator',\n",
       " 'outside',\n",
       " 'sea',\n",
       " 'pretty',\n",
       " 'p',\n",
       " 'read',\n",
       " 'used',\n",
       " 'log',\n",
       " 'savamob',\n",
       " 'hold',\n",
       " 'cum',\n",
       " 'whole',\n",
       " 'f',\n",
       " 'term',\n",
       " 'jay',\n",
       " 'unlimited',\n",
       " 'listen',\n",
       " 'fone',\n",
       " 'mrng',\n",
       " 'wkly',\n",
       " 'frm',\n",
       " 'support',\n",
       " 'course',\n",
       " 'sub',\n",
       " 'blue',\n",
       " 'hungry',\n",
       " 'seeing',\n",
       " 'ten',\n",
       " 'b4',\n",
       " 'frnds',\n",
       " 'ni8',\n",
       " 'case',\n",
       " 'meant',\n",
       " 'hmm',\n",
       " 'least',\n",
       " 'sunday',\n",
       " 'earlier',\n",
       " 'telling',\n",
       " 'fr',\n",
       " 'le',\n",
       " 'freemsg',\n",
       " 'cd',\n",
       " 'snow',\n",
       " 'talking',\n",
       " 'child',\n",
       " 'bslvyl',\n",
       " 'invited',\n",
       " 'motorola',\n",
       " 'computer',\n",
       " 'reading',\n",
       " 'almost',\n",
       " 'stupid',\n",
       " 'side',\n",
       " 'march',\n",
       " 'dnt',\n",
       " 'moment',\n",
       " 'joy',\n",
       " 'felt',\n",
       " 'txts',\n",
       " 'film',\n",
       " 'luck',\n",
       " 'hmmm',\n",
       " 'father',\n",
       " 'christmas',\n",
       " 'gas',\n",
       " 'balance',\n",
       " 'currently',\n",
       " 'single',\n",
       " 'mah',\n",
       " 'ago',\n",
       " 'lost',\n",
       " '87066',\n",
       " 'mr',\n",
       " 'press',\n",
       " 'park',\n",
       " '0800',\n",
       " 'understand',\n",
       " 'store',\n",
       " 'sex',\n",
       " 'rock',\n",
       " 'yar',\n",
       " 'paper',\n",
       " 'india',\n",
       " 'as',\n",
       " 'enter',\n",
       " 'area',\n",
       " 'hee',\n",
       " 'couple',\n",
       " 'un',\n",
       " 'happiness',\n",
       " 'pas',\n",
       " 'valued',\n",
       " '2003',\n",
       " 'etc',\n",
       " 'download',\n",
       " 'within',\n",
       " 'move',\n",
       " 'mayb',\n",
       " 'tired',\n",
       " 'cut',\n",
       " 'john',\n",
       " '03',\n",
       " 'return',\n",
       " 'direct',\n",
       " 'seen',\n",
       " 'sell',\n",
       " 'ac',\n",
       " 'via',\n",
       " 'complimentary',\n",
       " 'promise',\n",
       " 'eye',\n",
       " 'gym',\n",
       " 'grin',\n",
       " '12',\n",
       " 'supposed',\n",
       " '20',\n",
       " 'an',\n",
       " 'sending',\n",
       " 'reveal',\n",
       " 'laptop',\n",
       " 'darren',\n",
       " 'correct',\n",
       " 'askd',\n",
       " 'kid',\n",
       " 'heard',\n",
       " 'difficult',\n",
       " 'surprise',\n",
       " 'comp',\n",
       " 'confirm',\n",
       " 'rental',\n",
       " 'sort',\n",
       " 'redeemed',\n",
       " 'st',\n",
       " 'load',\n",
       " 'eh',\n",
       " 'knew',\n",
       " 'information',\n",
       " 'hospital',\n",
       " 'call2optout',\n",
       " 'lady',\n",
       " 'die',\n",
       " 'wow',\n",
       " 'photo',\n",
       " '04',\n",
       " 'ipod',\n",
       " 'swing',\n",
       " 'ugh',\n",
       " 'picking',\n",
       " 'semester',\n",
       " '200',\n",
       " 'bill',\n",
       " 'charged',\n",
       " 'hr',\n",
       " 'red',\n",
       " 'figure',\n",
       " 'extra',\n",
       " 'shower',\n",
       " 'met',\n",
       " 'leaf',\n",
       " 'blood',\n",
       " 'lover',\n",
       " 'page',\n",
       " 'wana',\n",
       " 'txting',\n",
       " 'safe',\n",
       " 'clean',\n",
       " 'police',\n",
       " 'door',\n",
       " 'train',\n",
       " 'bath',\n",
       " 'fantasy',\n",
       " 'nyt',\n",
       " 'joke',\n",
       " 'muz',\n",
       " 'asap',\n",
       " 'whenever',\n",
       " 'checking',\n",
       " 'orchard',\n",
       " 'kate',\n",
       " 'std',\n",
       " 'entered',\n",
       " 'study',\n",
       " 'usf',\n",
       " 'rply',\n",
       " 'cheer',\n",
       " 'idea',\n",
       " 'save',\n",
       " 'bcoz',\n",
       " 'road',\n",
       " 'laugh',\n",
       " 'request',\n",
       " 'reward',\n",
       " 'member',\n",
       " 'dogging',\n",
       " 'link',\n",
       " 'gn',\n",
       " 'loan',\n",
       " 'ex',\n",
       " 'abiola',\n",
       " 'small',\n",
       " 'wap',\n",
       " 'rent',\n",
       " 'slowly',\n",
       " 'w1j6hl',\n",
       " 'eg',\n",
       " 'comin',\n",
       " 'monday',\n",
       " 'asking',\n",
       " 'rest',\n",
       " 'lovable',\n",
       " 'hg',\n",
       " 'crave',\n",
       " 'remove',\n",
       " 'fact',\n",
       " 'discount',\n",
       " 'truth',\n",
       " 'wine',\n",
       " 'loved',\n",
       " 'somebody',\n",
       " 'slow',\n",
       " 'ringtones',\n",
       " 'pete',\n",
       " 'fantastic',\n",
       " 'reached',\n",
       " 'booked',\n",
       " 'gap',\n",
       " 'oops',\n",
       " 'immediately',\n",
       " 'teach',\n",
       " 'merry',\n",
       " 'sale',\n",
       " 'del',\n",
       " 'ho',\n",
       " 'different',\n",
       " 'kick',\n",
       " 'admirer',\n",
       " 'callertune',\n",
       " 'cover',\n",
       " 'copy',\n",
       " 't',\n",
       " 'woke',\n",
       " 'ldn',\n",
       " 'mm',\n",
       " 'glad',\n",
       " 'custcare',\n",
       " 'deal',\n",
       " 'pray',\n",
       " 'england',\n",
       " 'poor',\n",
       " 'rakhesh',\n",
       " 'otherwise',\n",
       " 'gettin',\n",
       " 'bag',\n",
       " 'representative',\n",
       " 'short',\n",
       " 'ntt',\n",
       " 'yep',\n",
       " 'getzed',\n",
       " 'sony',\n",
       " 'situation',\n",
       " 'nah',\n",
       " 'mistake',\n",
       " 'voice',\n",
       " 'summer',\n",
       " 'wishing',\n",
       " 'bathe',\n",
       " 'convey',\n",
       " 'energy',\n",
       " 'catch',\n",
       " 'men',\n",
       " 'hoping',\n",
       " 'em',\n",
       " 'across',\n",
       " 'med',\n",
       " 'empty',\n",
       " 'hw',\n",
       " 'deep',\n",
       " 'somewhere',\n",
       " 'worried',\n",
       " 'ge',\n",
       " '2nite',\n",
       " 'normal',\n",
       " 'doctor',\n",
       " 'hmv',\n",
       " 'cheap',\n",
       " 'write',\n",
       " 'tonite',\n",
       " 'gave',\n",
       " 'warm',\n",
       " '00',\n",
       " '900',\n",
       " 'il',\n",
       " 'opinion',\n",
       " 'doin',\n",
       " 'nobody',\n",
       " 'spend',\n",
       " 'water',\n",
       " 'turn',\n",
       " 'usual',\n",
       " 'dead',\n",
       " 'comuk',\n",
       " 'mrt',\n",
       " 'age16',\n",
       " 'charity',\n",
       " 'weed',\n",
       " 'tht',\n",
       " 'ive',\n",
       " 'mon',\n",
       " 'al',\n",
       " 'bid',\n",
       " '4u',\n",
       " 'hotel',\n",
       " 'urself',\n",
       " 'sick',\n",
       " 'wondering',\n",
       " 'forever',\n",
       " 'flag',\n",
       " 'excuse',\n",
       " 'lazy',\n",
       " 'staying',\n",
       " 'gay',\n",
       " 'doesnt',\n",
       " 'reaching',\n",
       " 'especially',\n",
       " 'studying',\n",
       " 'lect',\n",
       " 'trust',\n",
       " 'brings',\n",
       " 'password',\n",
       " 'bluetooth',\n",
       " 'rose',\n",
       " 'hiya',\n",
       " 'moral',\n",
       " 'using',\n",
       " 'none',\n",
       " 'tampa',\n",
       " 'colleague',\n",
       " 'fall',\n",
       " 'user',\n",
       " 'round',\n",
       " 'result',\n",
       " 'indian',\n",
       " 'bb',\n",
       " 'excellent',\n",
       " 'seems',\n",
       " 'accept',\n",
       " 'buying',\n",
       " 'completely',\n",
       " 'meh',\n",
       " '2day',\n",
       " 'sitting',\n",
       " 'style',\n",
       " 'kinda',\n",
       " 'unless',\n",
       " 'seriously',\n",
       " 'noon',\n",
       " 'sunshine',\n",
       " 'street',\n",
       " 'flight',\n",
       " 'ice',\n",
       " 'ldew',\n",
       " ...]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_model.wv.index_to_key #this will print list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "efce50f0-97a2-40d9-b986-a77b9f6eb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2vec_model.corpus_count  #total words count\n",
    "#w2vec_model.epochs  #how many epochs model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d2166b3-5512-41f1-9433-d696226f1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cash', 0.9990086555480957),\n",
       " ('prize', 0.9988747239112854),\n",
       " ('offer', 0.9988082051277161),\n",
       " ('awarded', 0.9988008737564087),\n",
       " ('customer', 0.998798668384552),\n",
       " ('mobile', 0.9987077116966248),\n",
       " ('service', 0.9986602663993835),\n",
       " ('landline', 0.998612105846405),\n",
       " ('latest', 0.9986038208007812),\n",
       " ('reply', 0.998502790927887)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_model.wv.similar_by_word('call') #vectors & words releated to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422faf3-7e78-4b76-bff9-0122b33da6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
